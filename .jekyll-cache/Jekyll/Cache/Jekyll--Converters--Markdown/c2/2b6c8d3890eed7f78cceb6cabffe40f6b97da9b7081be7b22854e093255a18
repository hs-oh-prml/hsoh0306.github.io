I"º:<h2 id="alexnet">AlexNet</h2>
<p>â€œImageNet Classification with Deep Convolutional Neural Networksâ€ì˜ ì €ì Alex Krizhevskyì˜ ì´ë¦„ì„ ë”°ì„œ networkì˜  AlexNetì´ë¼ëŠ” ëª…ì¹­ì´ ë¶™ì—ˆë‹¤. ILSVRC-2012ì—ì„œ top-5 test errorë¥¼ 15.3%ë¥¼ ê¸°ë¡ í–ˆìœ¼ë©°, 26.2%ì—ˆë˜ 2ë“±ê³¼ ë¹„êµí•˜ì—¬ ë¬´ë ¤ 10%ê°€ ë„˜ëŠ” ê²©ì°¨ë¥¼ ë‚´ê³  ìš°ìŠ¹í–ˆë‹¤. ReLUì˜ ëŒ€ìœ í–‰ì„ ë§Œë“¤ì–´ë‚¸ ì˜ë¯¸ ìˆëŠ” ëª¨ë¸ì´ë‹¤.</p>

<h2 id="dataset">Dataset</h2>
<p>ImageNetì€ ì•½ 22000 ê°€ì§€ì˜ classì— ì†í•˜ëŠ” 1500ë§Œê°œì˜ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ë°ì´í„° ì…‹ì´ë‹¤. ë°ì´í„°ëŠ” Amazonâ€™s Mechanical Turk crowd-sourcing toolë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ëŒì´ labelingí•˜ì—¬ ìˆ˜ì§‘ëœë‹¤ê³  í•œë‹¤. ILSVRC(ImageNet Large-Scale Visual Acception Challenge)ì—ì„œëŠ” ImageNetì—ì„œ ì•½ 1000ì¥ì˜ ì´ë¯¸ì§€ë¥¼ ê°€ì§„ 1000ê°œì˜ classë¥¼ ì‚¬ìš©í•œë‹¤. ì „ì²´ì ìœ¼ë¡œ 120ë§Œ ì¥ì˜ í•™ìŠµ ì´ë¯¸ì§€ì™€ 5ë§Œ ì¥ì˜ ê²€ì¦ ì´ë¯¸ì§€, 15ë§Œ ì¥ì˜ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ê°€ ìˆë‹¤. top-1ê³¼ top-5 errorë¥¼ ImageNetì— ë³´ê³ í•˜ëŠ”ë°, ì—¬ê¸°ì„œ top-5 errorëŠ” ëª¨ë¸ì´ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ê³  ì¶œë ¥í•œ 5 ê°œì˜ labelì— ëŒ€í•´ì„œ ì •ë‹µì´ ì—†ëŠ” ë¹„ìœ¨ì´ë‹¤. 
ImageNetì€ variable-resolution ì´ë¯¸ì§€ë¡œ êµ¬ì„±ë˜ì§€ë§Œ, constant input dimensionalityë¥¼ í•„ìš”ë¡œ ë•Œë¬¸ì—, ì´ë¯¸ì§€ë¥¼ 256x256ìœ¼ë¡œ down samplingì„ í–ˆë‹¤ê³  í•œë‹¤. ì´ë¯¸ì§€ë¥¼ ìë¥´ëŠ” ì‘ì—… ì™¸ì— ë‹¤ë¥¸ ì „ì²˜ë¦¬ë¥¼ í•˜ì§€ì•Šê³ , í”½ì…€ì˜ R,G,B ì›ë˜ì˜ ê°’ì„ ì‚¬ìš©í–ˆë‹¤.</p>

<h2 id="alexnet-network-architecture">AlexNet Network Architecture</h2>
<p>ë„¤íŠ¸ì›Œí¬ëŠ” 5ê°œì˜ convolutional layerì™€ 3ê°œì˜ fully-connected layerë¡œ ì´ 8ê°œì˜ layerë¡œ êµ¬ì„±ëœë‹¤. ë„¤íŠ¸ì›Œí¬ ì„¤ê³„ì— ì‚¬ìš©ëœ 4ê°œì˜ í° íŠ¹ì§•ì„ ì†Œê°œí•œë‹¤.</p>

<h3 id="relu-nonlinearity">ReLU Nonlinearity</h3>
<p>ReLUê°€ ìœ í–‰í•˜ê¸° ì „ê¹Œì§€ í™œì„±í™” í•¨ìˆ˜ëŠ” tanhê°€ ë§ì´ ì‚¬ìš©ëë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ê²½ì‚¬ í•˜ê°•ë²•ì—ì„œ saturating nonlinearitiesëŠ” non-saturating nonlinearityë³´ë‹¤ í›¨ì”¬ ëŠë¦¬ë‹¤ê³  í–ˆë‹¤. saturating nonlinearitiesì™€ non-saturating nonlinearityê°€ ë­”ì§€ ì°¾ì•„ë³´ë‹ˆ<br /></p>

<ul>
  <li>\(f\) is non-saturating iff \((\|\lim\limits_{z \to -\infty} f(z)\|= +\infty)âˆ¨(\|\lim\limits_{z \to \infty} f(z)\|= +\infty)\) <br /></li>
  <li>\(f\) is saturating iff \(f\) is not non-saturating.<br /></li>
</ul>

<p>\(f\)ê°€ ë¬´í•œìœ¼ë¡œ ë°œì‚°í•˜ë©´ non-saturatingì´ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ saturatingì´ë€ë‹¤. ì¦‰, ReLUëŠ” ë¬´í•œìœ¼ë¡œ ë°œì‚°í•˜ë¯€ë¡œ, non-saturatingì´ê³ , [-1, 1] ìˆ˜ë ´í•˜ëŠ” tanhë‚˜ [0, 1]ë¡œ ìˆ˜ë ´í•˜ëŠ” sigmoidëŠ” saturatingì— í•´ë‹¹ëœë‹¤.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Sigmoid</th>
      <th style="text-align: center">tanh</th>
      <th style="text-align: center">ReLU</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/assets/img/sigmoid.png" width="100%" /></td>
      <td style="text-align: center"><img src="/assets/img/tanh.png" width="90%" /></td>
      <td style="text-align: center"><img src="/assets/img/relu.png" width="100%" /></td>
    </tr>
    <tr>
      <td style="text-align: center">\(sigmoid(x) = (1+e^{-x})^{-1}\)</td>
      <td style="text-align: center">\(tanh(x) = (1-e^{-x})/(1+e^{-x})\)</td>
      <td style="text-align: center">\(ReLU(x) = max(0, x)\)</td>
    </tr>
  </tbody>
</table>

<p>Sigmoidì™€ tanhì— ë¹„í•´ì„œ ReLUì˜ ì‹ì´ ë‹¨ìˆœí•œ ê²ƒìœ¼ë¡œ ë³´ì•„, ì—°ì‚° ì†ë„ë¥¼ ë¹ ë¥´ê²Œ í•œë‹¤ëŠ” ê²ƒì„ ì§ê´€ì ìœ¼ë¡œ ì¶”ë¡ ì´ ê°€ëŠ¥í•œë°, ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<center>
<img src="/assets/img/alexnet_relu.png" width="60%" />
</center>

<p><em>Figure 1: A four-layer convolutional neural network with ReLUs (solid line) reaches a 25% training error rate on CIFAR-10 six times faster than an equivalent network with tanh neurons (dashed line). The learning rates for each network were chosen independently to make training as fast as possible. No regularization of any kind was employed. The magnitude of the effect demonstrated here varies with network architecture, but networks with ReLUs consistently learn several times faster than equivalents with saturating neurons.</em></p>

<p>ì‹¤ì„ ì´ ReLUë¥¼ ì‚¬ìš©í•œ ê²°ê³¼ê³ , ì ì„ ì€ tanhë¥¼ ì‚¬ìš©í•œ ì‹¤í—˜ ê²°ê³¼ë‹¤. ê²°ê³¼ë¡œ ë³´ì•˜ì„ ë•Œ, ReLUê°€ tanhë¥¼ ì‚¬ìš©í–ˆì„ ë•Œë³´ë‹¤ 6ë°° ì´ìƒ ë¹ ë¥¸ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ë ‡ê²Œ ë¹¨ë¼ì§„ ì†ë„ëŠ” ëŒ€ëŸ‰ì˜ í•™ìŠµ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê°€ëŠ¥í•˜ê²Œ í–ˆë‹¤.</p>

<h3 id="training-on-multiple-gpu">Training on Multiple GPU</h3>
<p>ì €ìëŠ” ì‹¤í—˜ì„ ìœ„í•´ GTX 580 GPUë¥¼ 2ê°œë¥¼ ì‚¬ìš©í–ˆë‹¤ê³  í•œë‹¤. 120ë§Œê°œì˜ í•™ìŠµ ë°ì´í„°ë¥¼ ì¶©ë¶„íˆ í•™ìŠµ ì‹œí‚¬ë§Œí•œ í¬ê¸°ì˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ë¡œë“œí•˜ëŠ”ë° 3GBì§œë¦¬ GPUí•˜ë‚˜ë¡œëŠ” ë¶€ì¡±í–ˆë‹¤. í˜„ëŒ€ì˜ GPUëŠ” í˜¸ìŠ¤íŠ¸ì˜ ë©”ëª¨ë¦¬ë¥¼ í†µí•˜ì§€ ì•Šê³ , ë‹¤ë¥¸ GPUë¡œ ì§ì ‘ I/Oë¥¼ í•  ìˆ˜ ìˆì–´ ë³‘ë ¬í™”ì— ì í•©í•˜ë‹¤. ë„¤íŠ¸í¬ë¥¼ ë°˜ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ê° GPUì— ë¡œë“œí•˜ì—¬, í•™ìŠµì„ ì§„í–‰í•œë‹¤. íŠ¹ì • ë ˆì´ì–´ì—ì„œëŠ” ê° GPUì˜ íŒŒë¼ë¯¸í„°ë¥¼ ëª¨ë‘ ë°›ìœ¼ë©° í†µì‹ ì„ í–ˆë‹¤ê³  í•œë‹¤. ì´ ë°©ë²•ì„ í†µí•´ì„œ top-1 errorë¥¼ 1.7%, top-5 errorë¥¼ 1.2% ê°ì†Œì‹œì¼°ìœ¼ë©°, ë‹¨ì¼ GPUë¥¼ ì‚¬ìš©í–ˆì„ ë•Œë³´ë‹¤ í•™ìŠµì‹œê°„ì„ ë” ë‹¨ì¶•ì‹œí‚¬ ìˆ˜ ìˆì—ˆë‹¤.</p>

<h3 id="local-response-normalization">Local Response Normalization</h3>

<blockquote>
  <p>ReLUs have the desirable property that they do not require input normalization to prevent them from saturating. If at least some training examples produce a positive input to a ReLU, learning will happen in that neuron. However, we still find that the following local normalization scheme aids generalization.</p>
</blockquote>

<p>ReLUëŠ” inputì— ëŒ€í•˜ì—¬ nomalizationì´ í•„ìš” ì—†ê³ , Local Response Normalizaationì€ generalizationì— ë„ì›€ì´ ëœë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆë‹¤ê³  í•œë‹¤. Local Response Normalizaationì€ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<blockquote>
  <center> $$b_{x,y}^{i} = a_{x,y}^{i} / (k+\alpha \displaystyle\sum_{j=max(0,i-n/2)}^{min(N-1,i+n/2)} (a_{x,y}^j)^{2})^\beta$$ </center>
  <p>where<br />
\(b_{x,y}^{i}\) : regularized output for kernel \(i\) at position \(x, y\)<br />
\(a_{x,y}^{i}\) : source output of kernel \(i\) applied at position \(x, y\)<br />
\(N\) : total number of kernels<br />
\(n\) : size of normalization neigborhood<br />
\(\alpha, \beta, (n)\) : hyperparmeters</p>
</blockquote>

<p>\(a_{x,y}^{i}\)ëŠ” inputìœ¼ë¡œ \(i\)ë²ˆì§¸ kernelì—ì„œ \(x, y\) ê°’ì„ ì˜ë¯¸í•˜ê³ , \(b_{x,y}^{i}\)ëŠ” outputì„ ì˜ë¯¸í•œë‹¤. ë…¼ë¬¸ì—ì„œëŠ” \(k=2, n=5, \alpha=10^{-4}, \beta=0.75\)ë¡œ ì„¤ì •í•˜ê³ , top-1 errorë¥¼ 1.4%, top-5 errorë¥¼ 1.2% ê°ì†Œì‹œí‚¤ëŠ” íš¨ê³¼ë¥¼ ì–»ì—ˆë‹¤ê³  í•œë‹¤.</p>

<h3 id="overlapping-pooling">Overlapping Pooling</h3>
<p>ì „í†µì ì¸ CNNì—ì„œëŠ” non-overlapping poolingì„ ì‚¬ìš©í–ˆì§€ë§Œ, overlapping poolingì„ í†µí•´ ì•½ê°„ì˜ overfitì„ ê°œì„ í•˜ëŠ”ë° íš¨ê³¼ë¥¼ ë´¤ë‹¤ê³  í•œë‹¤. overlapping poolingì€ kernelì˜ í¬ê¸°ë³´ë‹¤ strideì˜ í¬ê¸°ë¥¼ ì‘ê²Œí•˜ë©´ì„œ êµ¬í˜„ëœë‹¤. kernelì˜ í¬ê¸°ì™€ strideì˜ í¬ê¸°ê°€ ê°™ì€ ê²½ìš° non-overlapping poolingì´ ëœë‹¤. kernelì˜ í¬ê¸°ë³´ë‹¤, strideê°€ ì‘ê²Œí•˜ì—¬ ì‹œê°í™”í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤.</p>

<p><img src="/assets/img/overlapping_pooling.png" alt="" /></p>

<p>ë…¼ë¬¸ì—ì„œëŠ” ì´ ë°©ë²•ì„ ì´ìš©í•˜ì—¬ top-1, top-5 errorë¥¼ 0.4%, 0.3% ê°ì†Œì‹œì¼°ë‹¤ê³ í•œë‹¤.</p>

<h3 id="architecture">Architecture</h3>

<center>
<img src="/assets/img/alexnet.png" width="100%" />
</center>

<p><em>Figure 2: An illustration of the architecture of our CNN, explicitly showing the delineation of responsibilities between the two GPUs. One GPU runs the layer-parts at the top of the figure while the other runs the layer-parts at the bottom. The GPUs communicate only at certain layers. The networkâ€™s input is 150,528-dimensional, and the number of neurons in the networkâ€™s remaining layers is given by 253,440â€“186,624â€“64,896â€“64,896â€“43,264â€“4096â€“4096â€“1000.</em></p>

<p>ë„¤íŠ¸í¬ëŠ” 5ê°œì˜ convolution layerì™€ 3ê°œì˜ fully-connected layerë¡œ êµ¬ì„±ëœë‹¤. ë§ˆì§€ë§‰ fully-connected layerëŠ” softmaxë¥¼ í†µí•´ ê° 1000ê°œì˜ class labelì— ëŒ€í•œ í™•ë¥ ì„ ì¶œë ¥í•œë‹¤. 3ë²ˆì§¸ layerì™€ ëª¨ë“  fully-connected layerì—ì„œ GPU connectionì´ ì´ë£¨ì–´ì ¸ parameterë¥¼ ê³µìœ í•œë‹¤. Local Response Normalizationì€ ì²«ë²ˆì§¸ì™€ ë‘ë²ˆì§¸ layerì—ì„œ ì´ë£¨ì–´ì§„ë‹¤. ì²«ë²ˆì§¸ì™€ ë‘ë²ˆì§¸, ë‹¤ì„¯ë²ˆì§¸ layerì—ì„œ max-poolingì´ ëœë‹¤. ReLUëŠ” ëª¨ë“  layerì— ì ìš©ëœë‹¤. ì…ë ¥ ì´ë¯¸ì§€ì˜ í¬ê¸°ëŠ” 224x224x3ì´ë‹¤.</p>

<table>
  <tbody>
    <tr>
      <td>Convolution Layer 1</td>
      <td>96x(11x11x3) kernels</td>
      <td>Conv5x5, stride: 1 kernel: 256, padding: 2</td>
      <td>ReLU</td>
      <td>Overlapping Max pooling3x3, stride: 2</td>
      <td>Local Response Normalization</td>
      <td>Â </td>
    </tr>
    <tr>
      <td>Convolution Layer 2</td>
      <td>256x(5x5x48) kernels</td>
      <td>Conv3x3, stride: 1 kernel: 384, padding: 1</td>
      <td>ReLU</td>
      <td>Overlapping Max pooling3x3, stride: 2</td>
      <td>Local Response Normalization</td>
      <td>Â </td>
    </tr>
    <tr>
      <td>Convolution Layer 3</td>
      <td>384x(3x3x256) kernels</td>
      <td>Conv3x3, stride: 1 kernel: 384, padding: 1</td>
      <td>ReLU</td>
      <td>Â </td>
      <td>Â </td>
      <td>GPU Connection</td>
    </tr>
    <tr>
      <td>Convolution Layer 4</td>
      <td>384x(3x3x192) kernels</td>
      <td>Conv3x3, stride: 1 kernel: 384, padding: 1</td>
      <td>ReLU</td>
      <td>Â </td>
      <td>Â </td>
      <td>Â </td>
    </tr>
    <tr>
      <td>Convolution Layer 5</td>
      <td>256x(3x3x192) kernels</td>
      <td>Conv3x3, stride: 1 kernel: 256,padding: 1</td>
      <td>ReLU</td>
      <td>Overlapping Max pooling3x3, stride: 2</td>
      <td>Â </td>
      <td>Â </td>
    </tr>
    <tr>
      <td>Fully-Connected Layer 1</td>
      <td>4096 neurons</td>
      <td>Â </td>
      <td>ReLU</td>
      <td>Â </td>
      <td>Â </td>
      <td>GPU Connection</td>
    </tr>
    <tr>
      <td>Fully-Connected Layer 2</td>
      <td>4096 neurons</td>
      <td>Â </td>
      <td>ReLU</td>
      <td>Â </td>
      <td>Â </td>
      <td>GPU Connection</td>
    </tr>
    <tr>
      <td>Fully-Connected Layer 3</td>
      <td>1000 neurons</td>
      <td>Â </td>
      <td>Softmax</td>
      <td>Â </td>
      <td>Â </td>
      <td>GPU Connection</td>
    </tr>
  </tbody>
</table>

<p>ìš”ì•½í•˜ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<center>
<img src="/assets/img/alexnet_network_summary.png" width="100%" />
</center>

<h2 id="reducing-overfitting">Reducing Overfitting</h2>
<p>6000ë§Œ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•œë‹¤. ILSVRCì—ì„œëŠ” 1000ê°œì˜ classë¥¼ í•™ìŠµì‹œí‚¤ëŠ”ë° image-label mappingì„ 10 bitë¡œ ì œì•½í–ˆê³ , ì´ëŠ” ë§ì€ íŒŒë¼ë¯¸í„°ë¥¼ overfitting ì—†ì´ í•™ìŠµ ì‹œí‚¤ê¸° ë¶ˆì¶©ë¶„í•˜ë‹¤ê³  í–ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” 2ê°€ì§€ ë°©ë²•ì„ í†µí•´ overfittingì„ ë§‰ê³ ì í–ˆë‹¤.</p>

<h3 id="data-augmentation">Data Augmentation</h3>
<p>ê°€ì¥ ê°„ë‹¨í•˜ê²Œ overfiittingì„ ì¤„ì´ëŠ” ë°©ë²•ì€ ë°ì´í„°ì…‹ì„ ì¸ê³µì ìœ¼ë¡œ ëŠ˜ë¦¬ëŠ” ê²ƒì´ë‹¤. 
ë…¼ë¬¸ì—ì„œëŠ” 2ê°€ì§€ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ data augmentationì„ í–ˆë‹¤.</p>
<blockquote>
  <p>The first form of data augmentation consists of generating image translations and horizontal reflections.<br />
The second form of data augmentation consists of altering the intensities of the RGB channels in training images. Specifically, we perform PCA on the set of RGB pixel values throughout the ImageNet training set.</p>
</blockquote>

<p>ì²«ë²ˆì§¸ ë°©ë²•ì€ 256x256 ì´ë¯¸ì§€ì—ì„œ ì„ì˜ì˜ ë¶€ë¶„ì„ 224x224ë¡œ ìë¥´ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ê²Œ í•˜ë©´ ê°™ì€ labelì„ ê°€ì§„ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ë¡œ ì—¬ëŸ¬ ê°œì˜ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ ìˆ˜ ìˆê²Œ ëœë‹¤.</p>

<center>
<img src="/assets/img/alexnet_augmentation.png" width="100%" />
</center>

<p>256x256 í¬ê¸°ì˜ ì´ë¯¸ì§€ë¥¼ 224x224ë¡œ cropí•˜ê³ , ìˆ˜í‰ ë°˜ì „ì—ë„ ë˜‘ê°™ì´ ì ìš©í•˜ì—¬ ë°ì´í„°ê°€ 2048ë°° ì¦ê°€ ì‹œí‚¬ ìˆ˜ ìˆì—ˆê³ , ë„¤íŠ¸ì›Œí¬ì˜ ì…ë ¥ì´ 224x224ì¸ ì´ìœ ì´ë‹¤.</p>

<p>ë‘ë²ˆì§¸ ë°©ë²•ì€ PCAë¥¼ RGBì— ì ìš©í•˜ì—¬ augmentationì„ í–ˆë‹¤ê³  í•œë‹¤. PCA(Principal component analysis)ëŠ” ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì› ë°ì´í„°ë¡œ í™˜ì› ì‹œí‚¤ëŠ” ë°©ë²•ì´ë¼ê³  í•œë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ì¢€ ë” ê³µë¶€í•´ë´ì•¼ í•  ê²ƒ ê°™ë‹¤. RGBì— PCAë¥¼ ì ìš©í•˜ì—¬ ì´ë¯¸ì§€ì˜ eigenvalue \(\lambda_{i}\)ì™€ eigenvetor \(p_{i}\)ë¥¼ êµ¬í•˜ê³ , \(\lambda_{i}\)ì— í‰ê· ì´ 0ì´ê³  í‘œì¤€í¸ì°¨ê°€ 0.1ì¸ guassian ë¶„í¬ì˜ random variableì„ ê³±í•˜ì—¬ augmentationì„ í–ˆë‹¤ê³  í•œë‹¤.</p>

<blockquote>
  <center>$$I_{xy} = [I_{xy}^{R}, I_{xy}^{G},I_{xy}^{B}] + [p_{1}, p_{2}, p_{3}][\alpha_{1}\lambda_{1}, \alpha_{2}\lambda_{2}, \alpha_{3}\lambda_{3}]^{T}$$</center>
</blockquote>

<p>ì´ ë°©ë²•ì„ í†µí•´ì„œ top-1 error rateë¥¼ 1% ê°ì†Œí•˜ëŠ” íš¨ê³¼ë¥¼ ì–»ì—ˆë‹¤.</p>

<h3 id="dropout">Dropout</h3>
<p>Computation costë¥¼ ì¤„ì´ê³ , overfittingì„ ì¤„ì´ê¸° ìœ„í•´ dropoutì„ ì ìš©í–ˆë‹¤. dropoutì€ ê° hidden neuronì— ê°’ì„ 0ìœ¼ë¡œ í•˜ëŠ” í™•ë¥ ì„ 0.5ë¡œ ì„¸íŒ…í•˜ë©´ì„œ êµ¬í˜„í–ˆë‹¤ê³  í•œë‹¤. dropoutìœ¼ë¡œ êº¼ì§„ neuronì€ forward passì™€ back-propagationì— ê´€ì—¬í•˜ì§€ ì•ŠëŠ”ë‹¤. ë§¤ í•™ìŠµë§ˆë‹¤ êº¼ì§€ëŠ” neuronì€ ë‹¤ë¥´ì§€ë§Œ, weightëŠ” ê³µìœ í•œë‹¤ê³  í•œë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ëª¨ë“  neuronì„ ì‚¬ìš©í•˜ê³ , ì¶œë ¥ì— 0.5ë¥¼ ê³±í–ˆë‹¤.</p>

<blockquote>
  <p>At test time, we use all the neurons but multiply their outputs by 0.5, which is a reasonable approximation to taking the geometric mean of the predictive distributions produced by the exponentially-many dropout networks.</p>
</blockquote>

<center>
<img src="/assets/img/dropout_2014.png" />
</center>

<blockquote>
  <p>â€˜Dropout: A Simple Way to Prevent Neural Networks from Overfittingâ€™</p>
</blockquote>

<center>
<img src="/assets/img/dropout.png" />
</center>

<blockquote>
  <p>â€˜Improving neural networks by preventing co-adaptation of feature detectorsâ€™</p>
</blockquote>

<h3 id="result">Result</h3>

<center>
<img src="/assets/img/alexnet_result.png" />
</center>

<hr />
<h3 id="reference">Reference</h3>
<p><a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton,â€™ImageNet Classification with Deep Convolutional Neural Networksâ€™, <em>Neural Information Processing Systems (NIPS)</em>, 2012</a></p>

<p><a href="https://arxiv.org/pdf/1207.0580.pdf">G.E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever and R.R. Salakhutdinov, â€˜Improving neural networks by preventing co-adaptation of feature detectorsâ€™, 2012</a></p>

<p><a href="https://dl.acm.org/doi/pdf/10.5555/2627435.2670313">Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov, â€˜Dropout: A Simple Way to Prevent Neural Networks from Overfittingâ€™, vol.15, No.1, 2014</a></p>

<p><a href="https://stats.stackexchange.com/questions/174295/what-does-the-term-saturating-nonlinearities-mean">https://stats.stackexchange.com/questions/174295/what-does-the-term-saturating-nonlinearities-mean</a></p>
:ET